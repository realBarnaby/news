{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNfEo2fhw4r4U9OKn9FA9Zi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/realBarnaby/news/blob/main/news.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Szövegfeldolgozás:\n",
        "megadhatóak weboldalak,\n",
        "a program egy nlp segítségével leszedi ezeket,\n",
        "majd elküldi egy adott e-mailre\n",
        "\n",
        "továbbá készíthető hozzá egy webapp\n",
        "és egy adatbázis\n",
        "\n",
        "bónusz? személyre szabási lehetőségek esetleg elkészíthető több nlpre, weboldalra, rovatra"
      ],
      "metadata": {
        "id": "Yn_grUbT2muO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modulok"
      ],
      "metadata": {
        "id": "cQ4qMIwnTuS2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "llz2CmZAcjvO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fabb0ec-2224-40fc-fdd3-4ce8d66a57d3",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting newspaper3k\n",
            "  Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (4.12.3)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (9.4.0)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (6.0.1)\n",
            "Collecting cssselect>=0.9.2 (from newspaper3k)\n",
            "  Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (4.9.4)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (3.8.1)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (2.31.0)\n",
            "Collecting feedparser>=5.2.1 (from newspaper3k)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tldextract>=2.0.1 (from newspaper3k)\n",
            "  Downloading tldextract-5.1.2-py3-none-any.whl (97 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.6/97.6 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting feedfinder2>=0.0.4 (from newspaper3k)\n",
            "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jieba3k>=0.35.1 (from newspaper3k)\n",
            "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (2.8.2)\n",
            "Collecting tinysegmenter==0.3 (from newspaper3k)\n",
            "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.16.0)\n",
            "Collecting sgmllib3k (from feedparser>=5.2.1->newspaper3k)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (4.66.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (2024.2.2)\n",
            "Collecting requests-file>=1.4 (from tldextract>=2.0.1->newspaper3k)\n",
            "  Downloading requests_file-2.0.0-py2.py3-none-any.whl (4.2 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.13.4)\n",
            "Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13538 sha256=31d560e3e51c0b36cf06e837c1e199cc4593f9c2551bbe6e0e6ab67c3f4d73cd\n",
            "  Stored in directory: /root/.cache/pip/wheels/c8/d6/6c/384f58df48c00b9a31d638005143b5b3ac62c3d25fb1447f23\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3340 sha256=753f44144c3a55d92bf7a9c69712863db2434b3a0c0a9609e7eae18907bfa801\n",
            "  Stored in directory: /root/.cache/pip/wheels/97/02/e7/a1ff1760e12bdbaab0ac824fae5c1bc933e41c4ccd6a8f8edb\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398382 sha256=7b10313183ca642b29de926e2a79d1c45dc359f12ca07c3f273814c554c25f34\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/c4/0c/12a9a314ecac499456c4c3b2fcc2f635a3b45a39dfbd240299\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6049 sha256=34545451be0d2b9e6d0111c5f23e21dda7827d1e5f05ed7cc0bfc886be5e7147\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
            "Successfully built tinysegmenter feedfinder2 jieba3k sgmllib3k\n",
            "Installing collected packages: tinysegmenter, sgmllib3k, jieba3k, feedparser, cssselect, requests-file, feedfinder2, tldextract, newspaper3k\n",
            "Successfully installed cssselect-1.2.0 feedfinder2-0.0.4 feedparser-6.0.11 jieba3k-0.35.1 newspaper3k-0.2.8 requests-file-2.0.0 sgmllib3k-1.0.0 tinysegmenter-0.3 tldextract-5.1.2\n"
          ]
        }
      ],
      "source": [
        "# importáljuk a newspaper3k modult, amely segít gyűjteni és feldolgozni a híreket\n",
        "!pip install newspaper3k"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importáljuk a datetime modult, amely segít kezelni a dátumokat\n",
        "import datetime\n",
        "\n",
        "from newspaper import Article\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# importáljuk read timed out hibák elkerülése érdekében\n",
        "from newspaper import Config"
      ],
      "metadata": {
        "id": "CTaA31WxTyAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Origo"
      ],
      "metadata": {
        "id": "W94iEh8bPAF7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dátum beállítása\n",
        "# ---------------------------------------------------\n",
        "\n",
        "\n",
        "# létrehozunk egy üres listát az url-ek tárolására\n",
        "urls = []\n",
        "\n",
        "# létrehozunk egy datetime objektumot 2024. április 1-jével\n",
        "date = datetime.date(2024, 4, 1)  # megadva a felhasználó áltlal a webappban\n",
        "\n",
        "# létrehozunk egy másik datetime objektumot 2024. aprilis 2-ával\n",
        "end_date = datetime.date(2024, 4, 2)  # megadva a felhasználó áltlal a webappban\n",
        "\n",
        "# végigmegyünk a dátumokon egy napos lépésekkel, amíg el nem érjük a végdátumot\n",
        "while date < end_date:\n",
        "    # kinyerjük a dátum évét, hónapját és napját\n",
        "    year = date.year\n",
        "    month = date.month\n",
        "    day = date.day\n",
        "    # összerakjuk az url-t a dátummal\n",
        "    url = f\"https://www.origo.hu/hirarchivum/{year}/{month}/{day}\"\n",
        "    # hozzáadjuk az url-t a listához\n",
        "    urls.append(url)\n",
        "\n",
        "    # növeljük a dátumot egy nappal\n",
        "    date += datetime.timedelta(days=1)\n",
        "\n",
        "# kiírjuk az url-ek listáját\n",
        "print(urls)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "psT-EQqs2lub",
        "outputId": "9f33548b-a520-4279-e128-a1daee744b9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['https://www.origo.hu/hirarchivum/2024/4/1']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# hírek lekérése origon, a megszabott rovatok alapján\n",
        "# ---------------------------------------------------\n",
        "\n",
        "\n",
        "# egy lista a magyar politikai weboldalakról, amelyekről híreket szeretnél kapni\n",
        "# megadva a felhasználó áltlal a webappban\n",
        "\n",
        "# egy üres lista a hírek tárolására\n",
        "news = []                         # belföldi (politikai) hírek\n",
        "neutral_news = []                 # egyéb (nem annyira politikai) hírek\n",
        "\n",
        "# egy lista a választható rovatok tárolásásra\n",
        "preference = ['/itthon/', '/auto/', '/tudomany/', '/sport/']\n",
        "#print(\"preference: \", preference)\n",
        "input = preference[0]\n",
        "#print(\"input: \", input)\n",
        "\n",
        "# végigmegyünk a weboldalak listáján\n",
        "for website in urls:\n",
        "    # lekérjük a weboldal tartalmát\n",
        "    response = requests.get(website)\n",
        "    # létrehozunk egy BeautifulSoup objektumot a HTML elemzéséhez\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "    # megkeressük az összes hírcímet tartalmazó HTML elemet\n",
        "    links = soup.find_all(\"a\", href=True)\n",
        "\n",
        "    # segít elkerülni a Article download() failed with HTTPSConnectionPool port=443 Read timed out. (read timeout=7) on URL hibákat\n",
        "    config = Config()\n",
        "    config.request_timeout = 10\n",
        "\n",
        "    # végigmegyünk a hírcímeken\n",
        "    for link in links:\n",
        "\n",
        "      if link.find(class_=\"categorized-article-container-title\") is not None:\n",
        "        title = link.find(class_=\"categorized-article-container-title\").text\n",
        "    # kinyerjük a hírcím szövegét\n",
        "      href = link[\"href\"]\n",
        "\n",
        "      if href.startswith(input):\n",
        "          # létrehozunk egy Article objektumot a linkkel\n",
        "          article = Article(f'https://origo.hu'+href)\n",
        "          link = f'https://telex.hu' + article[\"href\"]\n",
        "\n",
        "          # letöltjük és elemezzük a cikk HTML-jét\n",
        "          try:\n",
        "            article.download()\n",
        "            article.parse()\n",
        "          # kinyerjük a cikk szövegét\n",
        "            text = article.text\n",
        "          # hozzáadjuk a hírt a listához egy szótár formájában\n",
        "            if (input == '/itthon/'):\n",
        "              news.append({\"title\": title, \"link\": f'https://origo.hu'+href, \"text\": text})\n",
        "              #print(\"news: \", news)\n",
        "            else:\n",
        "              neutral_news.append({\"title\": title, \"link\": f'https://origo.hu'+href, \"text\": text})\n",
        "              #print(\"neutral news: \", neutral_news)\n",
        "\n",
        "          except Exception as inst:\n",
        "            print(\"Hiba történt egy cikk letöltése vagy elemzése közben\")\n",
        "\n",
        "            print(type(inst))    # a hiba típusa\n",
        "\n",
        "            print(inst.args)     # arguments stored in .args    ez mit csinál pontosan?\n",
        "\n",
        "\n",
        "\n",
        "# létrehozunk egy pandas dataframe-et a hírek listájából\n",
        "df = pd.DataFrame(news)\n",
        "dfN = pd.DataFrame(neutral_news)\n",
        "\n",
        "# elmentjük a dataframe-et egy CSV fájlba\n",
        "df.to_csv(\"origo_news.csv\", index=False)\n",
        "dfN.to_csv(\"neutral_news.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "DFDMkajs9-hE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Telex"
      ],
      "metadata": {
        "id": "15frlCExTWbD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# a kinyerendő cikkek mennyiségének beállítása\n",
        "# ---------------------------------------------------\n",
        "\n",
        "\n",
        "# létrehozunk egy üres listát az url-ek tárolására\n",
        "urls = []\n",
        "# létrehozunk egy változót kezdőállapotnak\n",
        "start = 1\n",
        "# létrehozunk egy változót végállapotnak\n",
        "end = 3                     # egy napi adag cikk kb 8-9 oldal\n",
        "\n",
        "# végigmegyünk az oldalakon egy oldalas lépésekkel, amíg el nem érjük a végállapotot\n",
        "while start < end:\n",
        "    # átadjuk a kezdőállapotot a page_str-nek\n",
        "    page_str = f\"{start}\"\n",
        "    # összerakjuk az url-t az kezdő oldalszámmal\n",
        "    url = f\"https://telex.hu/archivum?oldal={page_str}\"\n",
        "    # hozzáadjuk az url-t a listához\n",
        "    urls.append(url)\n",
        "    # növeljük a kezdőállapotot egy nappal (a ciklus haladásának érdekében)\n",
        "    start += 1\n",
        "\n",
        "# kiírjuk az url-ek listáját\n",
        "print(urls)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lfZ_uq_aGrp",
        "outputId": "cdfa123a-f583-4531-df6b-12401c339d18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['https://telex.hu/archivum?oldal=1']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# hírek lekérése telexen\n",
        "# ---------------------------------------------------\n",
        "\n",
        "\n",
        "#url = 'https://telex.hu/archivum?oldal=1'\n",
        "# egy üres lista a hírek tárolására\n",
        "news = []                         # belföldi (politikai) hírek\n",
        "neutral_news = []                 # egyéb (nem annyira politikai) hírek\n",
        "\n",
        "# egy lista a választható rovatok tárolásásra\n",
        "preference = ['/belfold/', '/kult/', '/tudomany/', '/sport/']\n",
        "#print(\"preference: \", preference)\n",
        "input = preference[0]\n",
        "#print(\"input: \", input)\n",
        "\n",
        "# végigmegyünk a weboldalak listáján\n",
        "for website in urls:\n",
        "  try:\n",
        "      # lekérjük a weboldal tartalmát\n",
        "      response = requests.get(website)\n",
        "      response.raise_for_status()\n",
        "  # kiiratjuk a felvetülendő hibát és leállítjuk a programot\n",
        "  except requests.exceptions.RequestException as e:\n",
        "      print(e)\n",
        "      sys.exit(1)\n",
        "\n",
        "  # létrehozunk egy BeautifulSoup objektumot a HTML elemzéséhez\n",
        "  soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "  # megkeressük az összes hírcímet tartalmazó HTML elemet\n",
        "  articles = soup.find_all('a', class_=\"list__item__title\", href=True)\n",
        "\n",
        "  # segít elkerülni a Article download() failed with HTTPSConnectionPool port=443 Read timed out. (read timeout=7) on URL hibákat\n",
        "    config = Config()\n",
        "    config.request_timeout = 10\n",
        "\n",
        "  #df = pd.DataFrame(columns=['Title', 'Link', 'Text'])\n",
        "\n",
        "  for article in articles:\n",
        "      if article['href'].find('/belfold/') == 0:\n",
        "        title = article.find(class_=\"hasHighlight\").text.strip()\n",
        "        link = f'https://telex.hu' + article[\"href\"]\n",
        "        #df = df.append({'title': title, 'link': link}, ignore_index=True)\n",
        "        article = Article(link)\n",
        "        # letöltjük és elemezzük a cikk HTML-jét\n",
        "        try:\n",
        "          article.download()\n",
        "          article.parse()\n",
        "        # kinyerjük a cikk szövegét\n",
        "          text = article.text\n",
        "        # hozzáadjuk a hírt a listához egy szótár formájában\n",
        "        if (input == '/itthon/'):\n",
        "              news.append({\"title\": title, \"link\": f'https://origo.hu'+href, \"text\": text})\n",
        "              #print(\"news: \", news)\n",
        "            else:\n",
        "              neutral_news.append({\"title\": title, \"link\": f'https://origo.hu'+href, \"text\": text})\n",
        "              #print(\"neutral news: \", neutral_news)\n",
        "\n",
        "          except Exception as inst:\n",
        "            print(\"Hiba történt egy cikk letöltése vagy elemzése közben\")\n",
        "\n",
        "            print(type(inst))    # a hiba típusa\n",
        "\n",
        "            print(inst.args)     # arguments stored in .args    ez mit csinál pontosan?\n",
        "        news.append({\"title\": title, \"link\": link, \"text\": text})\n",
        "\n",
        "      elif article['href'].find('/kult/') == 0 or article['href'].find('/tudomany/') == 0 or article['href'].find('/sport/') == 0:\n",
        "        title = article.find(class_=\"hasHighlight\").text.strip()\n",
        "        link = f'https://telex.hu' + article[\"href\"]\n",
        "        #df = df.append({'title': title, 'link': link}, ignore_index=True)\n",
        "        article = Article(link)\n",
        "        # Letöltjük és elemezzük a cikk HTML-jét\n",
        "        article.download()\n",
        "        article.parse()\n",
        "        # Kinyerjük a cikk szövegét\n",
        "        text = article.text\n",
        "        # Hozzáadjuk a hírt a listához egy szótár formájában\n",
        "        neutral_news.append({\"title\": title, \"link\": link, \"text\": text})\n",
        "\n",
        "df = pd.DataFrame(news)\n",
        "dfN = pd.DataFrame(neutral_news)\n",
        "df.to_csv(\"telex_news.csv\", index=False)\n",
        "dfN.to_csv(\"neutral_news.csv\", index=False)"
      ],
      "metadata": {
        "id": "FFXhzUbLaM3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Index"
      ],
      "metadata": {
        "id": "yQOmt9WBT3OT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "df1RDNCmT64S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ripost"
      ],
      "metadata": {
        "id": "SCQoX8xT5DxO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x39A2NK-5-Pu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tárolás csv-ben"
      ],
      "metadata": {
        "id": "C9TjdfdlTbdD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "origo = pd.read_csv('origo_news.csv')\n",
        "origo['target'] = origo['target'] = 0\n",
        "telex = pd.read_csv('telex_news.csv')\n",
        "telex['target'] = telex['target'] = 1\n",
        "neutral = pd.read_csv('neutral_news.csv')\n",
        "neutral['target'] = neutral['target'] = 2"
      ],
      "metadata": {
        "id": "G8hpnf1JnzSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "allnews = pd.concat([origo, telex, neutral])\n",
        "allnews = allnews[['target','text']]\n",
        "allnews = allnews.dropna()"
      ],
      "metadata": {
        "id": "aTihRao-n2Pa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import pandas as pd\n",
        "allnews.to_csv(\"allnews.csv\", index=False)\n",
        "files.download('allnews.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "PtR_ao_in7Cj",
        "outputId": "57ea0df9-6cb2-4ad2-9db8-7ade9733ae45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_3dfaa35d-6f8e-406c-b4d9-4491c216fa8f\", \"allnews.csv\", 118652)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}